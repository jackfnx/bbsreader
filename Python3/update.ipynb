{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'E:/turboc/sis001'\n",
    "\n",
    "meta_data_path = os.path.join(save_path, 'meta.json')\n",
    "\n",
    "### BBS列表\n",
    "bbsdef = [\n",
    "    ['第一会所', 'http://www.sis001.com/forum/', 'forum-%d-%d.html', [383,]],\n",
    "    ['色中色', 'http://www.sexinsex.net/forum/', 'forum-%d-%d.html', [383,]],\n",
    "]\n",
    "\n",
    "### 参数\n",
    "if len(sys.argv) < 3:\n",
    "    sys.stderr.write('%r\\n' % bbsdef)\n",
    "    sys.exit(0)\n",
    "    \n",
    "# bbsId = int(sys.argv[1])\n",
    "# boardId = int(sys.argv[2])\n",
    "bbsId = 0\n",
    "boardId = 0\n",
    "\n",
    "_, base, start_page, boardIds = bbsdef[bbsId]\n",
    "curr_board = boardIds[boardId]\n",
    "\n",
    "### 如果存在json，load数据\n",
    "if os.path.exists(meta_data_path):\n",
    "    with open(meta_data_path) as f:\n",
    "        load_data = json.load(f)\n",
    "    last_timestamp = load_data['timestamp']\n",
    "    last_threads = load_data['threads']\n",
    "    tags = load_data['tags']\n",
    "    favorites = load_data['favorites']\n",
    "    blacklist = load_data['blacklist']\n",
    "### 如果不存在json，初始化空数据\n",
    "else:\n",
    "    last_timestamp = 0\n",
    "    last_threads = []\n",
    "    tags = {}\n",
    "    favorites = ['琼明神女录', '册母为后', '绿帽武林之淫乱后宫', '龙珠肏', '锦绣江山传']\n",
    "    blacklist = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 根据上次更新时间，确定这次读取几页\n",
    "now = time.time()\n",
    "if last_timestamp == 0:\n",
    "    pages = 100\n",
    "elif now - last_timestamp < 60*60*24*10: # 没超过10天，查看5页\n",
    "    pages = 5\n",
    "else:\n",
    "    pages = 30\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 获取html的爬虫类\n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self, delay):\n",
    "        \n",
    "        proxy_handler = request.ProxyHandler({\n",
    "            'http':'http://127.0.0.1:8080',\n",
    "            'https':'https://127.0.0.1.8080'\n",
    "        })\n",
    "        opener = request.build_opener(proxy_handler)\n",
    "        request.install_opener(opener)\n",
    "\n",
    "        self.delay = delay\n",
    "        self.last_accessed = time.time()\n",
    "\n",
    "    def getUrl(self, url):\n",
    "\n",
    "        if self.delay > 0:\n",
    "            sleep_secs = self.delay - (time.time() - self.last_accessed)\n",
    "\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs) \n",
    "        self.last_accessed = time.time()\n",
    "        \n",
    "        head = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36'\n",
    "        }\n",
    "\n",
    "        req = request.Request(url, headers=head)\n",
    "        response = request.urlopen(req)\n",
    "\n",
    "        html = response.read().decode('gbk')\n",
    "        print('Get [%s] OK' % url)\n",
    "        return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Thread对象\n",
    "def MakeThread(threadId, title, author, postTime, link):\n",
    "    dic = {\n",
    "        'threadId': threadId,\n",
    "        'title': title,\n",
    "        'author': author,\n",
    "        'postTime': postTime,\n",
    "        'link': link\n",
    "    }\n",
    "    return dic\n",
    "\n",
    "### 读取版面\n",
    "def bbsdoc(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    objs = soup.select('tbody[id^=stickthread_]')\n",
    "    objs += soup.select('tbody[id^=normalthread_]')\n",
    "    threads = []\n",
    "    for t in objs:\n",
    "        title = t.select('th span[id^=thread_] a')[0].text\n",
    "        link = t.select('th span[id^=thread_] a')[0]['href']\n",
    "        threadId = link.split('-')[1]\n",
    "        author = t.select('td.author cite a')[0].text\n",
    "        postTime = t.select('td.author em')[0].text\n",
    "        threads.append(MakeThread(threadId, title, author, postTime, link))\n",
    "    return threads\n",
    "\n",
    "### 读取文章\n",
    "def bbscon(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    postobj = soup.select('div[id^=postmessage_] div[id^=postmessage_]')[0]\n",
    "    [x.decompose() for x in postobj.select('strong')]\n",
    "    [x.decompose() for x in postobj.select('table')]\n",
    "    return postobj.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = Crawler(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get [http://www.sis001.com/forum/forum-383-1.html] OK\n",
      "Get [http://www.sis001.com/forum/forum-383-2.html] OK\n",
      "Get [http://www.sis001.com/forum/forum-383-3.html] OK\n",
      "Get [http://www.sis001.com/forum/forum-383-4.html] OK\n"
     ]
    }
   ],
   "source": [
    "### 读取新数据\n",
    "latest_threads = []\n",
    "for i in range(1, pages):\n",
    "    url = base + (start_path % (curr_board, i))\n",
    "    latest_threads += bbsdoc(crawler.getUrl(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 合并新旧数据\n",
    "def merge(lasts, latest):\n",
    "    threads = lasts[:]\n",
    "    append_threads = []\n",
    "    \n",
    "    lastIds = [x['threadId'] for x in lasts]\n",
    "    for t in latest:\n",
    "        if not t['threadId'] in lastIds:\n",
    "            threads.append(t)\n",
    "            append_threads.append(t)\n",
    "    return threads, append_threads\n",
    "\n",
    "threads, append_threads = merge(last_threads, latest_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword [琼明神女录] saved.\n",
      "keyword [册母为后] saved.\n",
      "keyword [绿帽武林之淫乱后宫] saved.\n",
      "keyword [龙珠肏] saved.\n",
      "keyword [锦绣江山传] saved.\n"
     ]
    }
   ],
   "source": [
    "### 扫描新增数据，提取关键字\n",
    "for t in append_threads:\n",
    "    threadId = t['threadId']\n",
    "    title = t['title']\n",
    "    keywords = re.findall('【(.*?)】', title)\n",
    "    for keyword in keywords:\n",
    "        if not keyword.isdigit():\n",
    "            if not keyword in tags:\n",
    "                tags[keyword] = []\n",
    "            tags[keyword].append(t)\n",
    "\n",
    "### 根据收藏夹，扫描所有文章，是否存在本地数据\n",
    "for fav in favorites:\n",
    "    if fav in tags:\n",
    "        for t in tags[fav]:\n",
    "            txtpath = os.path.join(save_path, t['threadId'] + '.txt')\n",
    "            if not os.path.exists(txtpath):\n",
    "                url = base + t['link']\n",
    "                chapter = bbscon(crawler.getUrl(url))\n",
    "                with open(txtpath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(chapter)\n",
    "        print('keyword [%s] saved.' % fav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 保存data\n",
    "with open(meta_data_path, 'w') as f:\n",
    "    save_data = {\n",
    "        'timestamp': time.time(),\n",
    "        'threads': threads,\n",
    "        'tags': tags,\n",
    "        'favorites': favorites,\n",
    "        'blacklist': blacklist,\n",
    "    }\n",
    "    json.dump(save_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
